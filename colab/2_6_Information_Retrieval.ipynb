{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "2.6_Information_Retrieval.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyMIaEg7F+LPlp+o+g6nTGzQ",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "spark-study2",
   "language": "python",
   "display_name": "spark-study2"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexander-n-thomas/spark-nlp-book-prod/blob/master/2_6_Information_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d4zpDO5R1ox4",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "outputId": "970f65f4-34fe-46ab-b959-c04b987dded9"
   },
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==2.5.1"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\r\n",
      "java version \"15.0.1\" 2020-10-20\r\n",
      "Java(TM) SE Runtime Environment (build 15.0.1+9-18)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 15.0.1+9-18, mixed mode, sharing)\r\n",
      "Collecting pyspark==2.4.4\r\n",
      "  Using cached pyspark-2.4.4-py2.py3-none-any.whl\r\n",
      "Collecting py4j==0.10.7\r\n",
      "  Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB)\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pyspark-stubs 3.0.0.post2 requires pyspark<3.1.0,>=3.0.0.dev0, but you have pyspark 2.4.4 which is incompatible.\u001B[0m\r\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.1\r\n",
      "Collecting spark-nlp==2.5.1\r\n",
      "  Using cached spark_nlp-2.5.1-py2.py3-none-any.whl (121 kB)\r\n",
      "Installing collected packages: spark-nlp\r\n",
      "Successfully installed spark-nlp-3.3.2\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HzgTx-7OuEnm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "! mkdir -p data"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dgBCSO9Fr-M5",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "outputId": "0e85f03f-0dc5-4999-a783-adbc9886dcd4"
   },
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-30 01:52:09--  https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz\r\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\r\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1860687 (1.8M) [application/x-httpd-php]\r\n",
      "Saving to: ‘mini_newsgroups.tar.gz.1’\r\n",
      "\r\n",
      "mini_newsgroups.tar 100%[===================>]   1.77M  1.34MB/s    in 1.3s    \r\n",
      "\r\n",
      "2021-11-30 01:52:12 (1.34 MB/s) - ‘mini_newsgroups.tar.gz.1’ saved [1860687/1860687]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CiqkXr6SITVJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "! tar xzf mini_newsgroups.tar.gz -C ./data/"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7hHyAtMQ-D_",
    "colab_type": "text"
   },
   "source": [
    "# Information Retrieval\n",
    "\n",
    "In the previous chapter we came across common words that made it difficult to characterize a corpus. This is a problem for different kinds NLP tasks. Fortunately, the field of information retrieval has developed many techniques that can be used to improve a variety of NLP applications.\n",
    "\n",
    "Earlier, we talked about how text data exists, and more is being generated every day. We need some way to manage and search through this data. If there is an ID or title, we can of course have an index on this data, but how do we search by content? With structured data, we can create logical expressions and retrieve all rows that satisfy the expressions. This can also be done with text, though less exactly.\n",
    "\n",
    "The foundation of information retrieval predates computers. Information retrieval focuses on how to find specific pieces of information in a larger set of information, especially information in text data. The most common type of task in information retrieval is search—in other words, document search.\n",
    "\n",
    "The following are the components of a document search:\n",
    "\n",
    "* Query $q$  \n",
    "A logical statement describing the document or kind of document you are looking for\n",
    "\n",
    "* Query term $q_t$  \n",
    "A term in the query, generally a token\n",
    "\n",
    "* Corpus of documents $D$  \n",
    "A collection of documents\n",
    "\n",
    "* Document $d$  \n",
    "A document in D with terms t_d that describe the document\n",
    "\n",
    "* Ranking function $r(q, D)$  \n",
    "A function that ranks the documents in D according to relevance to the query q\n",
    "\n",
    "* Result $R$  \n",
    "The ranked list of documents\n",
    "\n",
    "Before we get into how to implement these components, we need to consider a technical problem. How can we quickly access documents based on the information within them? If we have to scan every document, then we could not search large collections of documents. To solve this problem we use an inverted index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqYz60vUQ-b7",
    "colab_type": "text"
   },
   "source": [
    "## Inverted Indices\n",
    "\n",
    "Originally, indexing was a means of organizing and labeling information in a way that made retrieving it easier. For example, libraries use indexing to organize and find books. The Dewey Decimal Classification system is a way to index books based on their subject matter. We can also have indices based on titles, authors, publication dates, and so on. Another kind of index can often be found at the back of a book. This is a list of concepts in the book and pages on which to find them.\n",
    "\n",
    "The index in inverted index is slightly different than the traditional index; instead, it takes inspiration from the mathematical concept of indexing—that is, assigning indices to an element of a set. Recall our set of documents $D$. We can assign a number to each document, creating mapping from integers to documents, $i \\rightarrow d$.\n",
    "\n",
    "Let's create this index for our `DataFrame`. Normally, we would store an inverted index in a data store that allows for quick lookups. Spark `DataFrames` are not for quick lookups. We will introduce the tools used for search.\n",
    "\n",
    "## Building an Inverted Index\n",
    "\n",
    "Let's look at how we can build an inverted index in Spark. Here are the steps we will follow:\n",
    "\n",
    "1. Load the data.  \n",
    "\n",
    "2. Create the index: $i \\rightarrow d*$\n",
    "  * Since we are using Spark, we will generate this index on the rows.\n",
    "\n",
    "3. Process the text.\n",
    "\n",
    "4. Create the inverted index from terms to documents: $t_d \\rightarrow i*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsW1UTBKQ-kC",
    "colab_type": "text"
   },
   "source": [
    "### Step 1\n",
    "\n",
    "We will be creating an inverted index for the mini_newsgroups data set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aiWtsaeTR4Yf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import collect_set\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "spark = sparknlp.start()"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'print_exec' from 'pyspark.cloudpickle' (/Users/hyun.kim/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/cloudpickle/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-d50b46c05d75>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtypes\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcollect_set\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDDBarrier\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfiles\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkFiles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0maccumulators\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccumulators\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mAccumulator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbroadcast\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mBroadcast\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBroadcastPickleRegistry\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfiles\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkFiles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/broadcast.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mthreading\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcloudpickle\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mprint_exec\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_gateway\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlocal_connect_and_auth\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mserializers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mChunkedStream\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'print_exec' from 'pyspark.cloudpickle' (/Users/hyun.kim/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/cloudpickle/__init__.py)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4HQEGVgER6bW",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('data', 'mini_newsgroups', '*')\n",
    "texts = spark.sparkContext.wholeTextFiles(path)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('path', StringType()),\n",
    "    StructField('text', StringType()),\n",
    "])\n",
    "\n",
    "texts = spark.createDataFrame(texts, schema=schema).persist()"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-c5a43fadae7e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'mini_newsgroups'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'*'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtexts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwholeTextFiles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m schema = StructType([\n\u001B[1;32m      5\u001B[0m     \u001B[0mStructField\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'path'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mStringType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imVRaxuCQ-r7",
    "colab_type": "text"
   },
   "source": [
    "### Step 2\n",
    "\n",
    "Now we need to create the index. Spark assumes the data is distributed, so to assign an index, we need to use the lower-level `RDD` API. The zipWithIndex will sort the data on the workers and assign the indices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j85jHlBASBLg",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "outputId": "10aa3ccc-182b-47f5-dfd2-bf34a70ce34c"
   },
   "source": [
    "rows_w_indexed = texts.rdd.zipWithIndex()\n",
    "(path, text), i = rows_w_indexed.first()\n",
    "\n",
    "print(i)\n",
    "print(path)\n",
    "print(text[:200])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WUR1rWiQ-wk",
    "colab_type": "text"
   },
   "source": [
    "Now that we have created the index, we need to create a `DataFrame` like we did previously, except now we need to add our index into our `Rows`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_nwaG6BPSJ63",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "indexed = rows_w_indexed.map(\n",
    "    lambda row_index: Row(\n",
    "        index=row_index[1], \n",
    "        **row_index[0].asDict())\n",
    ")\n",
    "(i, path, text) = indexed.first()"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows_w_indexed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-84ac92dff0d5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m indexed = rows_w_indexed.map(\n\u001B[0m\u001B[1;32m      2\u001B[0m     lambda row_index: Row(\n\u001B[1;32m      3\u001B[0m         \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrow_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         **row_index[0].asDict())\n\u001B[1;32m      5\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'rows_w_indexed' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eQsPwY7dSLd-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "indexed_schema = schema.add(StructField('index', IntegerType()))\n",
    "\n",
    "indexed = spark.createDataFrame(indexed, schema=indexed_schema)\\\n",
    "    .persist()"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-ad75ee1a1046>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mindexed_schema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mStructField\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'index'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIntegerType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mindexed\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindexed_schema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mpersist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'schema' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_parse_memory' from 'pyspark.util' (/Users/hyun.kim/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/util.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-3c6ac4abcf70>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwindow\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mindexed\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtexts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"index\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrow_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"path\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDDBarrier\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfiles\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkFiles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStatusTracker\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkJobInfo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkStageInfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrddsampler\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRDDSampler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDDRangeSampler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDDStratifiedSampler\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstoragelevel\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStorageLevel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresource\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequests\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mExecutorResourceRequests\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTaskResourceRequests\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresource\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mResourceProfile\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresultiterable\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mResultIterable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/resource/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     20\u001B[0m \"\"\"\n\u001B[1;32m     21\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresource\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minformation\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mResourceInformation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresource\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequests\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTaskResourceRequest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTaskResourceRequests\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m     \u001B[0mExecutorResourceRequest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mExecutorResourceRequests\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresource\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mResourceProfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mResourceProfileBuilder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/resource/requests.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0m_parse_memory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name '_parse_memory' from 'pyspark.util' (/Users/hyun.kim/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/util.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "indexed = texts.withColumn(\"index\", row_number().over(Window.orderBy(\"path\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JnWtqnaNSM6v",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "outputId": "b5bed2b9-de01-40fa-b703-3cc9d2ea724d"
   },
   "source": [
    "indexed.limit(10).toPandas()"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-45d4746446d1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mindexed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlimit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'indexed' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N12fRsOvQ-19",
    "colab_type": "text"
   },
   "source": [
    "Each document $d$ is a collection of terms, $t_d$. So our index is the mapping from integers to collections of terms.\n",
    "\n",
    "An inverted index, on the other hand, is the mapping from terms $t_d$ to integers, $\\text{inv-index}: t_d \\rightarrow i, j, k, ...$ This allows us to quickly look up what documents contain a given term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLAncNAtQ-6s",
    "colab_type": "text"
   },
   "source": [
    "### Step 3\n",
    "\n",
    "Now let's process the text"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ClkoSGKlSnEf",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "outputId": "55bab768-425c-4cfc-cb9b-d278e34621f8"
   },
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "assembler = DocumentAssembler()\\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n",
    "lemmatizer = LemmatizerModel.pretrained()\\\n",
    "    .setInputCols(['token'])\\\n",
    "    .setOutputCol('lemma')\n",
    "normalizer = Normalizer()\\\n",
    "    .setInputCols(['lemma'])\\\n",
    "    .setOutputCol('normalized')\\\n",
    "    .setLowercase(True)\n",
    "finisher = Finisher()\\\n",
    "    .setInputCols(['normalized'])\\\n",
    "    .setOutputCols(['normalized'])\\\n",
    "    .setOutputAsArray(True)\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    assembler, tokenizer, \n",
    "    lemmatizer, normalizer, finisher\n",
    "]).fit(indexed)\n",
    "\n",
    "indexed_w_tokens = pipeline.transform(indexed)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'print_exec' from 'pyspark.cloudpickle' (/Users/hyun.kim/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/cloudpickle/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-b67a7bae7cbc>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0msparknlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpretrained\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPretrainedPipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0massembler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDocumentAssembler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0msetInputCol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'text'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0msetOutputCol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'document'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/sparknlp/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msparknlp\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mannotator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msparknlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbase\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDocumentAssembler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFinisher\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTokenAssembler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mChunk2Doc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDoc2Chunk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDDBarrier\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfiles\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkFiles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0maccumulators\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccumulators\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mAccumulator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbroadcast\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mBroadcast\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBroadcastPickleRegistry\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfiles\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkFiles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/broadcast.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mthreading\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcloudpickle\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mprint_exec\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_gateway\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlocal_connect_and_auth\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mserializers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mChunkedStream\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'print_exec' from 'pyspark.cloudpickle' (/Users/hyun.kim/anaconda3/envs/spark-study2/lib/python3.9/site-packages/pyspark/cloudpickle/__init__.py)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "knh3FlJqSoW3",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "outputId": "522a1b96-2537-4a49-e9c0-ff855a59ee58"
   },
   "source": [
    "indexed_w_tokens.limit(10).toPandas()"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexed_w_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-81e1aa361725>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mindexed_w_tokens\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlimit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'indexed_w_tokens' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nb1zNqo4SqOG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "doc_index = indexed_w_tokens.select('index', 'path', 'text').toPandas()\n",
    "doc_index = doc_index.set_index('index')"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexed_w_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-6dba26d8b8ba>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdoc_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexed_w_tokens\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'index'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'path'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'text'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdoc_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdoc_index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'index'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'indexed_w_tokens' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLvwjV1Q--9",
    "colab_type": "text"
   },
   "source": [
    "### Step 4\n",
    "\n",
    "Now, let us create our inverted index. We will use Spark SQL to do this.\n",
    "\n",
    "```\n",
    "SELECT term, collect_set(index) AS documents\n",
    "FROM (\n",
    "    SELECT index, explode(normalized) AS term\n",
    "    FROM indexed_w_tokens\n",
    ")\n",
    "GROUP BY term\n",
    "ORDER BY term\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mW_n6h4eSxSe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inverted_index = indexed_w_tokens\\\n",
    "    .selectExpr('index', 'explode(normalized) AS term')\\\n",
    "    .distinct()\\\n",
    "    .groupBy('term').agg(collect_set('index').alias('documents'))\\\n",
    "    .persist()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BXjWNWclSy91",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "outputId": "b84a97b3-2529-4710-f44c-4f2eead4903b"
   },
   "source": [
    "inverted_index.show(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlqREv6JQ_DG",
    "colab_type": "text"
   },
   "source": [
    "This is our inverted index. We can see that the term \"amplifier\" occurs in documents 630, 624, and 654. With this information, we can quickly find all documents that contain particular terms.\n",
    "\n",
    "Another benefit is that this inverted index is based on the size of our vocabulary, not on the amount of text in our corpus, so it is not big data. The inverted index grows only with new terms and document indices. For very large corpora, this can still be a large amount of data for a single machine. In the case of the mini_newsgroups data set, however, it is easily manageable.\n",
    "\n",
    "Let's see how big our inverted index is."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UoOvo464S14l",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "70bdf139-676f-46bd-9b08-4ccf5238b4e5"
   },
   "source": [
    "inverted_index.count()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19UFC0eIQ_HF",
    "colab_type": "text"
   },
   "source": [
    "For us, since we have such a small number of documents, the inverted index has more entries than the index.  Word frequencies follow Zipf's law—that is, the frequency of a word is inversely proportional to its rank when sorted. As a result, the most-used English words are already in our inverted index. This can be further constrained by not tracking words that don't occur at least a certain number of times."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LMhONbLUS4NO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inverted_index = {\n",
    "    term: set(docs) \n",
    "    for term, docs in inverted_index.collect()\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ5GPZwTQ_K1",
    "colab_type": "text"
   },
   "source": [
    " Now we can begin our most basic ranking function—simple Boolean search. In this case, let's look up all the documents that contain the words \"language\" or \"information.\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6zVLvGtgS7Sl",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "f91bee67-e03a-4874-a6f7-ac727e141a57"
   },
   "source": [
    "lang_docs = inverted_index['language']\n",
    "print('docs', ('{}, ' * 10).format(*list(lang_docs)[:10]), '...')\n",
    "print('number of docs', len(lang_docs))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "739eLjUxS88m",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "3099302f-9914-4b4c-8dfc-d7bfaacbd2a9"
   },
   "source": [
    "info_docs = inverted_index['information']\n",
    "print('docs', ('{}, ' * 10).format(*list(info_docs)[:10]), '...')\n",
    "print('number of docs', len(info_docs))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xh430LyfS-pu",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "91ff7f28-15c7-470d-c4bb-bea90acad6ae"
   },
   "source": [
    "filter_set = list(lang_docs | info_docs)\n",
    "print('number of docs in filter set', len(filter_set))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d8DNwQXpS__H",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "c435310d-d912-4cc3-a7b0-e53873a683ea"
   },
   "source": [
    "intersection = list(lang_docs & info_docs)\n",
    "print('number of docs in intersection set', len(intersection))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmEqFVo9Q_Of",
    "colab_type": "text"
   },
   "source": [
    "Let's print out lines from our filter set. Here, the filter set is the result set, but generally, the filter set is ranked by $r(q, D)$, which results in the result set.\n",
    "\n",
    "Let's look at the lines in which we see the occurrences, to get an idea about our result set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W9v526f1TD4-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "outputId": "5e30bfc3-6d5d-42d8-b331-eac286b848a7"
   },
   "source": [
    "k = 1\n",
    "for i in filter_set:\n",
    "    path, text = doc_index.loc[i]\n",
    "    lines = text.split('\\n')\n",
    "    print(path.split('/')[-1], 'length:', len(text))\n",
    "    for line_number, line in enumerate(lines):\n",
    "        if 'information' in line or 'language' in line:\n",
    "            print(line_number, line)\n",
    "    print()\n",
    "    k += 1\n",
    "    if k > 5:\n",
    "        break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hBgMW0fQ_R-",
    "colab_type": "text"
   },
   "source": [
    "Now that we have our result set, how should we rank our results? We could just count the number of occurrences of our search term, but that would be biased toward long documents. Also, what happens if our query includes a very common word like \"the\"? If we just use the counts, common words like \"the\" will dominate our results. In our result set, the one with the most occurrences of the query terms has the longest text. We could say that the more terms found in the document, the more relevant the document is, but this has problems too. What do we do with one-term queries? In our example, only one document has both. Again, if our query has a common word—for example, \"the cat in the hat\"—should \"the\" and \"in\" have the same importance as \"cat\" and \"hat\"? To solve this problem, we need a more flexible model for our documents and queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcqXlrEdQ_V7",
    "colab_type": "text"
   },
   "source": [
    "## Vector Space Model\n",
    "\n",
    "In the previous chapter, we introduced the concept of vectorizing documents. We talked about creating binary vectors, where 1 means that the word is present in the document. We can also use the counts.\n",
    "\n",
    "When we convert a corpus to a collection of vectors, we are implicitly modeling our language as a vector space. In this vector space, each dimension represents one term. This has many benefits and drawbacks. It is a simple way to represent our text in a manner that allows machine learning algorithms to work with it. It also allows us to represent the vectors sparsely. On the other hand, we lose the information contained in the word order. This process also creates high dimensional data sets, which can be problematic to some algorithms.\n",
    "\n",
    "Let's calculate the vectors for our data set. In the previous chapter, we used the `CountVectorizer` for this. We will build the vectors in Python, but the way we will build them will help us understand how libraries implement vectorization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AJbqMiTBTOsO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class SparseVector(object):\n",
    "    \n",
    "    def __init__(self, indices, values, length):\n",
    "        # if the indices are not in ascending order, we need \n",
    "        # to sort them\n",
    "        is_ascending = True\n",
    "        for i in range(len(indices) - 1):\n",
    "            is_ascending = is_ascending and indices[i] < indices[i+1]\n",
    "        if not is_ascending:\n",
    "            pairs = zip(indices, values)\n",
    "            sorted_pairs = sorted(pairs, key=lambda x: x[0])\n",
    "            indices, values = zip(*sorted_pairs)\n",
    "        self.indices = indices\n",
    "        self.values = values\n",
    "        self.length = length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            return self.values[self.indices.index(index)]\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "        \n",
    "    def dot(self, other):\n",
    "        assert isinstance(other, SparseVector)\n",
    "        assert self.length == other.length\n",
    "        res = 0\n",
    "        i = j = 0\n",
    "        while i < len(self.indices) and j < len(other.indices):\n",
    "            if self.indices[i] == other.indices[j]:\n",
    "                res += self.values[i] * other.values[j]\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif self.indices[i] < other.indices[j]:\n",
    "                i += 1\n",
    "            elif self.indices[i] > other.indices[j]:\n",
    "                j += 1\n",
    "        return res\n",
    "    \n",
    "    def hadamard(self, other):\n",
    "        assert isinstance(other, SparseVector)\n",
    "        assert self.length == other.length\n",
    "        res_indices = []\n",
    "        res_values = []\n",
    "        i = j = 0\n",
    "        while i < len(self.indices) and j < len(other.indices):\n",
    "            if self.indices[i] == other.indices[j]:\n",
    "                res_indices.append(self.indices[i])\n",
    "                res_values.append(self.values[i] * other.values[j])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif self.indices[i] < other.indices[j]:\n",
    "                i += 1\n",
    "            elif self.indices[i] > other.indices[j]:\n",
    "                j += 1\n",
    "        return SparseVector(res_indices, res_values, self.length)\n",
    "    \n",
    "    def sum(self):\n",
    "        return sum(self.values)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'SparseVector({}, {})'.format(\n",
    "            dict(zip(self.indices, self.values)), self.length)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfZZAgFsQ_Z8",
    "colab_type": "text"
   },
   "source": [
    "We need to make two passes over all the documents. In the first pass, we will get our vocabulary and the counts. In the second pass we will construct the vectors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bO4hXavnTTQe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = set()\n",
    "vectors = {}\n",
    "\n",
    "for row in indexed_w_tokens.toLocalIterator():\n",
    "    counts = Counter(row['normalized'])\n",
    "    vocabulary.update(counts.keys())\n",
    "    vectors[row['index']] = counts\n",
    "    \n",
    "vocabulary = list(sorted(vocabulary))\n",
    "inv_vocabulary = {term: ix for ix, term in enumerate(vocabulary)}\n",
    "vocab_len = len(vocabulary)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmzjxgYtQ_d8",
    "colab_type": "text"
   },
   "source": [
    "Now that we have this information, we need to go back over our word counts and construct actual vectors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BxpqjYKuTVjm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "for index in vectors:\n",
    "    terms, values = zip(*vectors[index].items())\n",
    "    indices = [inv_vocabulary[term] for term in terms]\n",
    "    vectors[index] = SparseVector(indices, values, vocab_len)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_zSfqJf3TXI-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "71b443a5-bcec-49b7-dc29-0262d7402655"
   },
   "source": [
    "vectors[42]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_BihQzI6Taq6",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "2172799e-8e48-4047-869e-e87331a1085f"
   },
   "source": [
    "vocabulary[3598]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DZ8VfqNXTcUd",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "b0b7ea75-7a19-4306-8d88-2a7964b5241b"
   },
   "source": [
    "vocabulary[37876]"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-314404741853>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mvocabulary\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m37876\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'vocabulary' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_2i8BqgQ_hu",
    "colab_type": "text"
   },
   "source": [
    "As we discussed previously, there are many drawbacks to using only the counts for a search. The concern is that words that are generally common in English will have more impact than the less common words. There are a couple strategies for addressing this. First, let's look at the simplest solution—removing the common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuHdIzCnQ_lr",
    "colab_type": "text"
   },
   "source": [
    "### Stop-Word Removal\n",
    "\n",
    "These common words we are looking to remove are called stop words.  This term was coined in the 1950s by Hans Peter Luhn, a pioneer in information retrieval. Default stop-word lists are available, but it is often necessary to modify generic stop-word lists for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qERfCRj_Tijm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sw_remover = StopWordsRemover() \\\n",
    "    .setInputCol(\"normalized\") \\\n",
    "    .setOutputCol(\"filtered\") \\\n",
    "    .setStopWords(StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "\n",
    "filtered = sw_remover.transform(indexed_w_tokens)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2pzc3jqKTj42",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary_filtered = set()\n",
    "vectors_filtered = {}\n",
    "\n",
    "for row in filtered.toLocalIterator():\n",
    "    counts = Counter(row['filtered'])\n",
    "    vocabulary_filtered.update(counts.keys())\n",
    "    vectors_filtered[row['index']] = counts\n",
    "    \n",
    "vocabulary_filtered = list(sorted(vocabulary_filtered))\n",
    "inv_vocabulary_filtered = {\n",
    "    term: ix \n",
    "    for ix, term in enumerate(vocabulary_filtered)\n",
    "}\n",
    "vocab_len_filtered = len(vocabulary)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5st3pBFDTlMl",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "outputId": "a0893c49-486f-41e5-f669-6851389a854d"
   },
   "source": [
    "for index in vectors:\n",
    "    terms, values = zip(*vectors_filtered[index].items())\n",
    "    indices = [inv_vocabular_filteredy[term] for term in terms]\n",
    "    vectors_filtered[index] = \\\n",
    "        SparseVector(indices, values, vocab_len_filtered)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oRTfosZHTmiV",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "vectors[42]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9KBokm-ITngm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "vocabulary[3264]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "23RFfV9gToUs",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "vocabulary[38226]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-g1WnOdTqxq",
    "colab_type": "text"
   },
   "source": [
    "The words \"bake\" and \"timmons\" seem more informative. You should explore your data when determining what words should be included in the stop-word list.\n",
    "\n",
    "It may seem like a daunting task to list all the words we don't want. However, recalling what we discussed about morphology, we can narrow down what we want to remove. We want to remove unbound function morphemes.\n",
    "\n",
    "A fluent speaker of a language, who knows these basics of morphology, is able to create a reasonably good list. However, this still leaves two concerns. What if we need to keep some common words? What if we want to remove some common lexical morphemes? You can modify the list, but that still leaves one last concern. How do we handle queries like \"fictional cats\"? The word \"fictional\" is less common than \"cats,\" so it makes sense that the former should be more important in determining what documents are returned. Let's look at how we can implement this using our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWQz-sPCTtXK",
    "colab_type": "text"
   },
   "source": [
    "## Inverse Document Frequency\n",
    "\n",
    "Instead of manually editing our vocabulary, we can try and weight the words. We need to find some way of weighting the words using their \"commonness.\" One way to define \"commonness\" is by identifying the number of documents in our corpus that contain the word.  This is generally called document frequency. We want words with high document frequency to be down-weighted, so we are interested in using inverse document frequency (IDF).\n",
    "\n",
    " We take these values and multiply them by the term frequencies, which are the frequencies of words in a given document.  The result of multiplying inverse document frequency by term frequency gives us the TF.IDF.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned} \n",
    "tf(t, d) &= \\text{the number of times } t \\text{ occurs in } d\\\\ \n",
    "df(t) &= \\text{the number of documents } t \\text{ occurs in }\\\\ \n",
    "idf(t) &= \\frac{\\text{the number of documents}}{df(t)} \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "There are many different flavors of TF.IDF. The most common kind is smoothed logarithmic.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned} \n",
    "tf(t, d) &= log(1 + \\text{the number of times } t \\text{ occurs in } d)\\\\ \n",
    "df(t) &= \\text{the number of documents } t \\text{ occurs in }\\\\ \n",
    "idf(t) &= log(\\frac{\\text{the number of documents}}{1+df(t)}) \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let's calculate this with our vectors. We actually already have the term frequency, so all we need to do is calculate the idf, transform the values with log, and multiply tf and idf."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zterCIAPTpeW",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "idf = Counter()\n",
    "\n",
    "for vector in vectors.values():\n",
    "    idf.update(vector.indices)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mJ7zqoyyUA2n",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "outputId": "e46fcb83-7121-4a09-dbf2-b27d5c2bee32"
   },
   "source": [
    "for ix, count in idf.most_common(20):\n",
    "    print('{:5d} {:20s} {:d}'.format(ix, vocabulary[ix], count))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSbzK0weUC_T",
    "colab_type": "text"
   },
   "source": [
    "We can now make `idf` a `SparseVector`. We know it contains all the words, so it actually won't be sparse, but this will help us implement the next steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1BgzEwQqUB3n",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "indices, values = zip(*idf.items())\n",
    "idf = SparseVector(indices, values, vocab_len)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BlIvNuCYUJkM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from math import log\n",
    "\n",
    "for index, vector in vectors.items():\n",
    "    vector.values = list(map(lambda v: log(1+v), vector.values))\n",
    "    \n",
    "idf.values = list(map(lambda v: log(vocab_len / (1+v)), idf.values))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wjSH7dhCULF2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tfidf = {index: tf.hadamard(idf) for index, tf in vectors.items()}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YYr-VMsJUM41",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "a0bdd35e-97a3-41d1-a33f-bd839d03f295"
   },
   "source": [
    "tfidf[42]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18NRqZaUUOzz",
    "colab_type": "text"
   },
   "source": [
    "Let's look at the TF.IDF values for \"be\" and \"the.\" Let's also look at one of the terms with a higher TF.IDF than these common words."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9JnDCgsTUN5O",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "c99cc4b7-6a25-4f5f-d21a-5bfdd322c113"
   },
   "source": [
    "tfidf[42][3598] # be"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s_frfQ5EURjm",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "efe5500f-e03d-4c0c-8323-d74ac05b83f2"
   },
   "source": [
    "tfidf[42][37876] # the"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qqaZmS5aUSrF",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "0ffdbb0e-9439-47d3-abee-61d236a762ab"
   },
   "source": [
    "vocabulary[17236], tfidf[42][17236]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjo1dMRlUWy5",
    "colab_type": "text"
   },
   "source": [
    "Let's look at the document to get an idea of why this word is so important."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q0DCjrBNUUBU",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "outputId": "abb31829-dac8-4f52-ae6d-d0a54033f8c8"
   },
   "source": [
    "print(doc_index.loc[42]['text'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vizSpsjWUZuw",
    "colab_type": "text"
   },
   "source": [
    "We can see the document is talking about some person named \"Maddi Hausman.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kokWBdX3Uc3O",
    "colab_type": "text"
   },
   "source": [
    "## In Spark\n",
    "Spark has stages for calculating TF.IDF in MLlib. If you have a column that contains arrays of strings, you can use either `CountVectorizer`, which we are already familiar with, or `HashingTF` to get the `tf` values.   `HashingTF` uses the hashing trick, in which you decide on a vector space beforehand, and hash the words into that vector space. If there is a collision, then those words will be counted as the same. This lets you trade off between memory efficiency and accuracy. As you make your predetermined vector space larger, the output vectors become larger, but the chance of collisions decreases.  \n",
    "\n",
    "Now that we know how to turn a document into a vector, in the next chapter we can explore how we can use that vector in classic machine learning tasks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVWVYGpPUdQV",
    "colab_type": "text"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Now that we have calculated the TF.IDF values, let's build a search function. First, we need a function to process the query."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AQuCca3fUo4V",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def process_query(query, pipeline):\n",
    "    data = spark.createDataFrame([(query,)], ['text'])\n",
    "    return pipeline.transform(data).first()['normalized']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBhECZ7uUeAc",
    "colab_type": "text"
   },
   "source": [
    "Then we need a function to get the filter set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3CVea6-GUrsc",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_filter_set(processed_query):\n",
    "    filter_set = set()\n",
    "    # find all the documents that contain any of the terms\n",
    "    return filter_set"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cRDfFiYUeQE",
    "colab_type": "text"
   },
   "source": [
    "Next, we need a function that will compute the score for the document."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uilzw8wNUuak",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_score(index, terms):\n",
    "    return # return a single score"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifsg1LrRUeZe",
    "colab_type": "text"
   },
   "source": [
    "We also want a function for displaying results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CS4jATYIUx0e",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def display(index, score, terms):\n",
    "    hits = [term for term in terms if term in vocabulary and tfidf[index][inv_vocabulary[term]] > 0.]\n",
    "    print('terms', terms, 'hits', hits)\n",
    "    print('score', score)\n",
    "    print('path', path)\n",
    "    print('length', len(doc_index.loc[index]['text']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EVZ6Co8UeGP",
    "colab_type": "text"
   },
   "source": [
    "Finally, we are ready for our search function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xK5BeFNU0r8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def search(query, pipeline, k=5):\n",
    "    processed_query = process_query(query, pipeline)\n",
    "    filter_set = get_filter_set(processed_query)\n",
    "    scored = {index: get_score(index, processed_query) for index in filter_set}\n",
    "    display_list = list(sorted(filter_set, key=scored.get, reverse=True))[:k]\n",
    "    for index in display_list:\n",
    "        display(index, scored[index], processed_query)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6H-rH9u9U26Z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "search('search engine', pipeline)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymWO9bM7Udnq",
    "colab_type": "text"
   },
   "source": [
    "You should be able to implement `get_filter_set` and `get_score` easily using examples in this chapter. Try out a few queries. You will likely notice that there are two big limitations here. There is no N-gram support, and the ranker is biased toward longer documents. What could you modify to fix these problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYQLGzG3UdD7",
    "colab_type": "text"
   },
   "source": [
    "## Resources\n",
    "\n",
    "* An Introduction to Information Retrieval, by Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze: this book covers many important aspects of information retrieval. Two of its three authors are also authors of Foundations of Statistical Natural Language Processing.\n",
    "* Apache Lucene: this is the most-used open source search engine. Often, one of the search platforms built on top of Lucene are used, Apache Solr or Elasticsearch.\n",
    "* Lucene in Action, 2nd ed., by Michael McCandless, Erik Hatcher, and Otis Gospodnetic (Manning Publications)\n",
    "A guide to implementing searches using Lucene\n",
    "* Elasticsearch: The Definitive Guide, by Clinton Gormley and Zachary Tong (O'Reilly)\n",
    "* A guide to implementing searches using Elasticsearch\n",
    "* Learning to Rank for Information Retrieval, by Tie-Yan Liu (Springer)\n",
    "  * Learning to rank, building machine learning–based rankers, is an important part of modern search engines. Tie-Yan Liu is one the most important contributors to the field of learning to rank. "
   ]
  }
 ]
}